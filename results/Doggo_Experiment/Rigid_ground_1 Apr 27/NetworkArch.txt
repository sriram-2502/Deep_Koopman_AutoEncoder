enc layers: 7
enc units [16,32,64,64,16,16,16], act:[tanh,tanh,tanh,tanh,tanh,tanh,tanh]
Koopman Layer: 1 
Koopman units: 16, act: None
dec layers: 5
dec units [64,64,32,64,2], act: [tanh,tanh,tanh,tanh,tanh]

training paramters
epochs: 10,000
learning_rate: 0.0001

Loss: alpha1 = 1, alpha2 = 1e-7, alpha3 = 1e-15

batch_size = 256
trajectory_Length = 64
NumTraj_batch = 4
Num_batches_train = 10
Num_batches_val = 1

states used: theta, theta_dot, gamma, gamma_dot
scaling for preprocess: 0 to 1

